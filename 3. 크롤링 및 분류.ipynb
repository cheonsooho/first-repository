{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 형태소 분석기 변경해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphs_processor name =             Hannanum, 0.28355 secs\n",
      "morphs_processor name =                 Kkma, 4.16372 secs\n",
      "morphs_processor name =              Komoran, 0.02187 secs\n",
      "morphs_processor name =                  Okt, 1.88283 secs\n",
      "morphs_processor name =                Mecab, 0.00099 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from konlpy.tag import Hannanum, Kkma, Komoran, Okt, Mecab\n",
    "tokenizer = [('Hannanum', Hannanum()), ('Kkma', Kkma()), ('Komoran', Komoran()), ('Okt', Okt()), ('Mecab', Mecab())]\n",
    "kor_texts = [kor_text]\n",
    "for name, tokenizer in tokenizer:\n",
    "    strat_time = time.time()\n",
    "    morphs = [tokenizer.morphs(kor_text) for kor_text in kor_texts]                                          \n",
    "    elapsed_time = time.time() - strat_time\n",
    "    print('morphs_processor name = %20s, %.5f secs' % (name, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[밤, 에, 귀가, 하, 던, 여성, 에게, 범죄, 를, 시도, 하, ㄴ, 대, 남성, 이, 구속, 되, 었, 다, 서울, 제주, 경찰서, 는, 상해, 혐의, 로, 씨, 를, 구속, 하, 아, 수사, 하, 고, 있, 다고, 일, 밝히, 었, 다, 씨, 는, 지난달, 일, 피해, 여성, 을, 인근, 지하철, 역, 에서부터, 따라가, 아, 폭행, 을, 시도, 하, 려다가, 도망가, ㄴ, 혐의, 를, 받, 는다, 피하, 아, 여성, 이, 저항, 하, 자, 놀라, ㄴ, 씨, 는, 도망가, 았, 으며, 신고, 를, 받, 고, 주변, 을, 수색, 하, 던, 경찰, 에, 체포, 되, 었, 다, 피하, 아, 여성, 은, 이, 과정, 에서, 경미, 하, ㄴ, 부상, 을, 입, 은, 것, 으로, 전하, 아, 지, 었, 다]\n",
      "komoran\n",
      "[밤, 에, 귀가, 하, 던, 여성, 에게, 범죄, 를, 시도, 하, ㄴ, 대, 남성, 이, 구속됐다서울, 제주경찰, 서는, 상하, 어, 혐의, 로, 씨, 를, 구속해, 수사, 하고, 있, 다, 고, 일, 밝혔다씨, 는, 지난달, 일, 피하, 어, 여성, 을, 인근, 지하철, 역, 에서부터, 따르, 아, 가, 아, 폭행, 을, 시도, 하, 려, 다가, 도망가, ㄴ, 혐의, 를, 받는다피해, 여성, 이, 저항, 하, 자, 놀라, ㄴ, 씨, 는, 도망가, 아며, 신고, 를, 받, 고, 주변, 을, 수색, 하, 던, 경찰, 에, 체포됐다피해, 여성, 은, 이, 과정, 에서, 경미한, 부상, 을, 입, 은, 것, 으로, 전하, 어, 지, 었다]\n",
      "Hannanum\n",
      "[밤, 에, 귀가, 하던, 여성, 에게, 범죄, 를, 시도, 한, 대, 남성, 이, 구속, 됐다, 서울, 제, 주, 경찰서, 는, 상해, 혐의, 로, 씨, 를, 구속, 해, 수사, 하고, 있다고, 일, 밝혔다, 씨, 는, 지난달, 일, 피해, 여성, 을, 인근, 지하철, 역, 에서부터, 따라가, 폭행, 을, 시도, 하려다가, 도망간, 혐의, 를, 받는다, 피해, 여성, 이, 저항, 하자, 놀란, 씨, 는, 도망갔으며, 신고, 를, 받고, 주변, 을, 수색, 하던, 경찰, 에, 체포, 됐다, 피해, 여성, 은, 이, 과정, 에서, 경미한, 부상, 을, 입은, 것, 으로, 전해졌다]\n",
      "Okt\n",
      "[밤, 에, 귀가, 하, 더, ㄴ, 여성, 에게, 범죄, 를, 시도, 하, ㄴ, 대, 남성, 이, 구속, 되, 었, 다, 서울, 제주, 경찰서, 는, 상해, 혐의, 로, 씨, 를, 구속, 하, 어, 수사, 하, 고, 있, 다고, 일, 밝히, 었, 다, 씨, 는, 지난달, 일, 피해, 여성, 을, 인근, 지하철, 역, 에서, 부터, 따라가, 폭행을, 시도, 하, 려, 다그, 아, 도망가, ㄴ, 혐의, 를, 받, 는, 다, 피해, 여성, 이, 저항, 하, 자, 놀라, ㄴ, 씨, 는, 도망가, 었, 으며, 신고, 를, 받, 고, 주변, 을, 수색, 하, 더, ㄴ, 경찰, 에, 체포, 되, 었, 다, 피해, 여성, 은, 이, 과정, 에서, 경미, 하, ㄴ, 부상, 을, 입, 은, 것, 으로, 전하, 어, 지, 었, 다]\n",
      "Kkma\n",
      "[밤, 에, 귀가, 하, 더, ㄴ, 여성, 에게, 범죄, 를, 시도, 하, ㄴ, 대, 남성, 이, 구속, 되, 었, 다, 서울, 제주, 경찰서, 는, 상해, 혐의, 로, 씨, 를, 구속, 하, 어, 수사, 하, 고, 있, 다고, 일, 밝히, 었, 다, 씨, 는, 지난달, 일, 피해, 여성, 을, 인근, 지하철, 역, 에서, 부터, 따라가, 폭행을, 시도, 하, 려, 다그, 아, 도망가, ㄴ, 혐의, 를, 받, 는, 다, 피해, 여성, 이, 저항, 하, 자, 놀라, ㄴ, 씨, 는, 도망가, 었, 으며, 신고, 를, 받, 고, 주변, 을, 수색, 하, 더, ㄴ, 경찰, 에, 체포, 되, 었, 다, 피해, 여성, 은, 이, 과정, 에서, 경미, 하, ㄴ, 부상, 을, 입, 은, 것, 으로, 전하, 어, 지, 었, 다]\n",
      "Mecab\n"
     ]
    }
   ],
   "source": [
    "class List(list): \n",
    "    def __str__(self): \n",
    "        return \"[\" + \", \".join([\"%s\" % x for x in self]) + \"]\"\n",
    "\n",
    "komoran= Komoran()\n",
    "print(List(komoran.morphs(kor_text)))\n",
    "print(\"komoran\")\n",
    "\n",
    "hannanum= Hannanum()\n",
    "print(List(hannanum.morphs(kor_text)))\n",
    "print(\"Hannanum\")\n",
    "\n",
    "okt= Okt()\n",
    "print(List(okt.morphs(kor_text)))\n",
    "print(\"Okt\")\n",
    "\n",
    "kkma= Kkma()\n",
    "print(List(kkma.morphs(kor_text)))\n",
    "print(\"Kkma\")\n",
    "\n",
    "Mecab= Mecab()\n",
    "print(List(kkma.morphs(kor_text)))\n",
    "print(\"Mecab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속도를 비교 하였을 때, Mecab가 가장 빨랐으며(0.00099), Kkma가 가장 느렸다(4.16372).\n",
    "# 정확성을 기준으로 komaoran과 Hannanum이 원문가 가장 비슷하였다.\n",
    "# 상기 결과는 모든 가능성을 내포 하지는 않는다.()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 불용어 추가해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤', '에', '귀가', '하', '던', '여성', '에게', '범죄', '를', '시도', '한', '대', '남성', '이', '구속', '됐', '다', '서울', '제주', '경찰서', '는', '상해', '혐의', '로', '씨', '를', '구속', '해', '수사', '하', '고', '있', '다고', '일', '밝혔', '다', '씨', '는', '지난달', '일', '피해', '여성', '을', '인근', '지하철', '역', '에서부터', '따라가', '폭행', '을', '시도', '하', '려다가', '도망간', '혐의', '를', '받', '는다', '피해', '여성', '이', '저항', '하', '자', '놀란', '씨', '는', '도망갔으며', '신고', '를', '받', '고', '주변', '을', '수색', '하', '던', '경찰', '에', '체포', '됐', '다', '피해', '여성', '은', '이', '과정', '에서', '경미', '한', '부상', '을', '입', '은', '것', '으로', '전해졌', '다']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#tokenizer = Mecab() -> 상기 '1. 형태소 분석기 변경해 보기'에서 호출\n",
    "print(tokenizer.morphs(kor_text))\n",
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 추가는 '1', 형태소 출력은 '2', 그만하시려면 '3'을 입력후 Enter를 누르세요.\n",
      "1\n",
      "의\n",
      "['에', '는', '은', '을', '했', '에게', '있', '이', '의', '하', '한', '다', '과', '때문', '할', '수', '무단', '따른', '및', '금지', '전재', '경향신문', '기자', '는데', '가', '등', '들', '파이낸셜', '저작', '등', '뉴스', '가', '가', '는', '가', '가', '는', '의']\n",
      "불용어 추가는 '1', 형태소 출력은 '2', 그만하시려면 '3'을 입력후 Enter를 누르세요.\n",
      "2\n",
      "['밤', '', '', '귀', '', '', '던', '', '여', '성', '', '게', '', '범', '죄', '를', '', '시', '도', '', '', '대', '', '남', '성', '', '', '구', '속', '됐', '', '서', '울', '', '제', '주', '경', '찰', '서', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '상', '해', '', '혐', '', '로', '', '씨', '를', '', '구', '속', '해', '', '', '사', '', '고', '', '', '', '고', '', '일', '', '밝', '혔', '', '씨', '', '', '지', '난', '달', '', '일', '', '피', '해', '', '여', '성', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '인', '근', '', '지', '', '철', '', '역', '', '서', '부', '터', '', '따', '라', '', '', '폭', '행', '', '', '시', '도', '', '려', '', '', '', '도', '망', '간', '', '혐', '', '를', '', '받', '', '', '피', '해', '', '', '', '', '', '', '', '', '', '', '', '', '', '여', '성', '', '', '저', '항', '', '자', '', '놀', '란', '', '씨', '', '', '도', '망', '갔', '으', '며', '', '신', '고', '를', '', '받', '고', '', '주', '변', '', '', '', '색', '', '던', '', '경', '찰', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '체', '포', '됐', '', '피', '해', '', '여', '성', '', '', '', '', '', '정', '', '서', '', '경', '미', '', '', '부', '상', '', '', '입', '', '', '것', '으', '로', '', '전', '해', '졌', '']\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"불용어 추가는 '1', 형태소 출력은 '2' 입력후 Enter를 누르세요.\")\n",
    "    if 1 == int(input()):\n",
    "        inputtext = input()\n",
    "        stopwords.append(inputtext)\n",
    "        print(stopwords)\n",
    "    else:\n",
    "        text_data = preprocessing(kor_text)\n",
    "        print(text_data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 다른 날짜 데이터 추가해 보기(실행 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러를 만들기 전 임포트\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "    news = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist\n",
    "\n",
    "url_list = make_urllist(2, 101, 20200611)\n",
    "url_list[:5]\n",
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "\n",
    "def make_data(urllist, code):\n",
    "  text_list = []\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text_list.append(article.text)\n",
    "\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  return df\n",
    "\n",
    "data = make_data(url_list, 101)\n",
    "code_list = [102, 103, 105]\n",
    "\n",
    "def make_total_data(page_num, code_list, date):\n",
    "  df = None\n",
    "\n",
    "  for code in code_list:\n",
    "    url_list = make_urllist(page_num, code, date)\n",
    "    df_temp = make_data(url_list, code)\n",
    "    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "  return df\n",
    "\n",
    "df = make_total_data(1, code_list, 20200611)\n",
    "\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "  print('{} File Saved!'.format(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
