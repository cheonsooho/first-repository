{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11-7. 프로젝트: 멋진 작사가 만들기\n",
    "\n",
    "##### Step 1. 데이터 다운로드\n",
    "___\n",
    "먼저 아래 링크에서 Song Lyrics 데이터를 다운로드해 주세요! 저장된 파일을 압축 해제한 후, 모든 txt 파일을 lyrics 폴더를 만들어 그 속에 저장해주세요!\n",
    "\n",
    "* [Song Lyrics](https://www.kaggle.com/paultimothymooney/poetry/data)  \n",
    "아래의 명령어를 실행하셔도 됩니다.\n",
    "\n",
    "```\n",
    "wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2. 데이터 읽어오기\n",
    "___\n",
    "glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['The first words that come out', 'And I can see this song will be about you', \"I can't believe that I can breathe without you\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your hands high what\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3. 데이터 정제\n",
    "___\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "**preprocess_sentence()** 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 문장을 **토큰화 했을 때 토큰의 개수가 15개**를 넘어가면 잘라내기를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) <= 5: continue\n",
    "    corpus.append(preprocess_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the first words that come out <end>\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   6 241 ...   0   0   0]\n",
      " [  2   8   5 ...   0   0   0]\n",
      " [  2   5  32 ...   0   0   0]\n",
      " ...\n",
      " [  2 240   1 ...   0   0   0]\n",
      " [  2  10 502 ...   0   0   0]\n",
      " [  2 129  20 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f8efdbd0290>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ', oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   6 241 424  17  67  56   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "tensors = []\n",
    "\n",
    "for i in range(len(tensor)):\n",
    "    if tensor[i][1] == tensor[i][2]: continue\n",
    "    if tensor[i][2] == tensor[i][3]: continue\n",
    "    tensors.append(tensor[i,:])\n",
    "\n",
    "print(tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = np.array(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   6 241 424  17  67  56   3   0   0   0   0   0   0]\n",
      "[  6 241 424  17  67  56   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "x_train = tensors[:, :-1]\n",
    "y_train = tensors[:, 1:]\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4. 평가 데이터셋 분리\n",
    "훈련 데이터와 평가 데이터를 분리하세요!\n",
    "\n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!\n",
    "\n",
    "```\n",
    "enc_train, enc_val, dec_train, dec_val = <코드 작성>\n",
    "```\n",
    "\n",
    "여기까지 올바르게 진행했을 경우, 아래 실행 결과를 확인할 수 있습니다.\n",
    "\n",
    "```\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "```\n",
    "\n",
    "```\n",
    "out:\n",
    "\n",
    "Source Train: (124960, 14)\n",
    "Target Train: (124960, 14)\n",
    "```\n",
    "\n",
    "만약 결과가 다르다면 천천히 과정을 다시 살펴 동일한 결과를 얻도록 하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = enc_train[:124960]\n",
    "dec_train = dec_train[:124960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124960, 14)\n",
      "Target Train: (124960, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(x_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5. 인공지능 만들기\n",
    "___\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!\n",
    "\n",
    "```\n",
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "```\n",
    "\n",
    "데이터가 커서 훈련하는 데 시간이 제법 걸릴 겁니다. 여유를 가지고 작업하시면 좋아요 :)\n",
    "\n",
    "```\n",
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-7.7590157e-05,  5.9371734e-05, -9.9167257e-05, ...,\n",
       "         -7.5823576e-05,  5.8274953e-05,  2.0059713e-04],\n",
       "        [-3.5009021e-04, -6.6554476e-06, -7.3485455e-05, ...,\n",
       "          8.1538076e-07,  1.3442989e-04,  2.4812450e-04],\n",
       "        [-6.7999307e-04,  2.9468918e-04, -3.0101993e-04, ...,\n",
       "          2.4471950e-04,  6.0984516e-05,  3.9856564e-04],\n",
       "        ...,\n",
       "        [ 8.6344208e-04,  9.6043007e-04, -8.0057612e-04, ...,\n",
       "          5.2677497e-04, -6.0627202e-04, -3.9102859e-04],\n",
       "        [ 1.0193330e-03,  9.4258948e-04, -7.5159653e-04, ...,\n",
       "          7.4957887e-04, -6.2180770e-04, -5.8369484e-04],\n",
       "        [ 9.3325455e-04,  8.6145604e-04, -8.0714579e-04, ...,\n",
       "          7.1952899e-04, -5.1546452e-04, -8.6031161e-04]],\n",
       "\n",
       "       [[-7.7590157e-05,  5.9371734e-05, -9.9167257e-05, ...,\n",
       "         -7.5823576e-05,  5.8274953e-05,  2.0059713e-04],\n",
       "        [-1.9743078e-05, -2.4274697e-04, -2.0811986e-04, ...,\n",
       "         -2.1020265e-04,  2.0639798e-04,  2.2468045e-04],\n",
       "        [ 8.4519466e-05, -7.9202306e-05, -3.2572207e-04, ...,\n",
       "         -2.2737012e-04,  3.8761963e-04,  1.2318563e-04],\n",
       "        ...,\n",
       "        [-1.4974582e-03,  7.1176665e-04,  1.3027762e-03, ...,\n",
       "          7.1538473e-04, -9.4547181e-04, -9.8663382e-05],\n",
       "        [-1.6183454e-03,  5.9327180e-04,  1.5882738e-03, ...,\n",
       "          1.0885387e-03, -1.0988634e-03, -7.9664038e-05],\n",
       "        [-1.7042723e-03,  4.7404508e-04,  1.8297504e-03, ...,\n",
       "          1.4607344e-03, -1.2025441e-03, -9.0099784e-05]],\n",
       "\n",
       "       [[-7.7590157e-05,  5.9371734e-05, -9.9167257e-05, ...,\n",
       "         -7.5823576e-05,  5.8274953e-05,  2.0059713e-04],\n",
       "        [-5.9444621e-05,  1.4891358e-04,  1.4159575e-04, ...,\n",
       "         -1.5409454e-04, -5.0366994e-05,  4.4644825e-05],\n",
       "        [-1.7897147e-04,  1.7103020e-04,  1.2810448e-04, ...,\n",
       "         -3.0809923e-04, -1.1250885e-04, -2.3444736e-04],\n",
       "        ...,\n",
       "        [-1.5463625e-03,  1.9346406e-04,  2.2855592e-03, ...,\n",
       "          2.2970242e-03, -1.0652974e-03, -5.4126856e-04],\n",
       "        [-1.5901816e-03,  1.5277094e-04,  2.4245733e-03, ...,\n",
       "          2.5653329e-03, -1.0371394e-03, -5.9533492e-04],\n",
       "        [-1.6245655e-03,  1.1838501e-04,  2.5438026e-03, ...,\n",
       "          2.8002639e-03, -9.9884800e-04, -6.5875053e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-7.7590157e-05,  5.9371734e-05, -9.9167257e-05, ...,\n",
       "         -7.5823576e-05,  5.8274953e-05,  2.0059713e-04],\n",
       "        [-1.7974689e-04,  2.1458293e-04,  1.0814638e-04, ...,\n",
       "          1.9526768e-04,  9.8551836e-06,  3.0164546e-04],\n",
       "        [-7.7269142e-05, -8.7460001e-05,  3.7480606e-04, ...,\n",
       "         -2.1196227e-04,  3.1376543e-04,  3.9594580e-04],\n",
       "        ...,\n",
       "        [-1.3946068e-04,  3.0154327e-04,  3.0037112e-04, ...,\n",
       "         -3.5806739e-04, -3.9142769e-04,  9.0758322e-04],\n",
       "        [-2.4093034e-04,  3.7251550e-04,  3.7835300e-04, ...,\n",
       "         -2.9657502e-04, -5.0395686e-04,  7.2485843e-04],\n",
       "        [-4.0099159e-04,  4.5452200e-04,  5.9271767e-04, ...,\n",
       "         -1.0468006e-04, -6.5922807e-04,  6.2759005e-04]],\n",
       "\n",
       "       [[-7.7590157e-05,  5.9371734e-05, -9.9167257e-05, ...,\n",
       "         -7.5823576e-05,  5.8274953e-05,  2.0059713e-04],\n",
       "        [-3.1747541e-04,  2.1654920e-04, -1.3856610e-04, ...,\n",
       "         -2.3574558e-04,  1.0179603e-04,  4.9671985e-04],\n",
       "        [-3.9769543e-04,  1.5447642e-04, -2.9321169e-04, ...,\n",
       "         -6.5244018e-04,  1.6429625e-04,  6.7264365e-04],\n",
       "        ...,\n",
       "        [-1.4209659e-03, -1.8623078e-04,  1.4814729e-03, ...,\n",
       "          1.0862557e-03, -1.2568464e-03, -1.0111657e-04],\n",
       "        [-1.4960109e-03, -1.7123930e-04,  1.7775004e-03, ...,\n",
       "          1.5024504e-03, -1.2652016e-03, -1.3373316e-04],\n",
       "        [-1.5521022e-03, -1.5998317e-04,  2.0273819e-03, ...,\n",
       "          1.8827968e-03, -1.2498505e-03, -1.9013412e-04]],\n",
       "\n",
       "       [[ 8.0720078e-05,  2.1762855e-04,  1.8459462e-04, ...,\n",
       "          5.2244723e-05, -7.2653493e-05, -1.6396541e-04],\n",
       "        [-6.1962339e-05,  8.6980645e-04,  4.1993344e-04, ...,\n",
       "         -1.6008552e-04, -1.7212183e-04,  1.4302284e-04],\n",
       "        [-2.1469679e-04,  1.1272726e-03,  5.3883548e-04, ...,\n",
       "         -4.4770396e-04, -1.8249558e-04,  1.8048960e-04],\n",
       "        ...,\n",
       "        [-2.4725098e-04, -4.1832638e-04, -4.1146867e-04, ...,\n",
       "         -2.7878970e-04, -5.9000286e-04, -2.4059846e-04],\n",
       "        [-1.7782136e-04, -3.5801379e-04, -4.8453349e-04, ...,\n",
       "         -7.9392019e-05, -6.4548099e-04, -3.0726471e-04],\n",
       "        [-1.7154978e-04, -3.9628777e-04, -5.9493212e-04, ...,\n",
       "          1.6612778e-04, -6.9572730e-04, -5.3413952e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3905/3905 [==============================] - 130s 33ms/step - loss: 3.3163\n",
      "Epoch 2/10\n",
      "3905/3905 [==============================] - 138s 35ms/step - loss: 2.8599\n",
      "Epoch 3/10\n",
      "3905/3905 [==============================] - 137s 35ms/step - loss: 2.5703\n",
      "Epoch 4/10\n",
      "3905/3905 [==============================] - 137s 35ms/step - loss: 2.3080\n",
      "Epoch 5/10\n",
      "3905/3905 [==============================] - 137s 35ms/step - loss: 2.0760\n",
      "Epoch 6/10\n",
      "3905/3905 [==============================] - 137s 35ms/step - loss: 1.8710\n",
      "Epoch 7/10\n",
      "3905/3905 [==============================] - 137s 35ms/step - loss: 1.6919\n",
      "Epoch 8/10\n",
      "3905/3905 [==============================] - 137s 35ms/step - loss: 1.5376\n",
      "Epoch 9/10\n",
      "3905/3905 [==============================] - 123s 31ms/step - loss: 1.4066\n",
      "Epoch 10/10\n",
      "3905/3905 [==============================] - 122s 31ms/step - loss: 1.2979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8efd785d10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love ma little nasty girl <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 실력 미숙으로 인하여, In [8], In [9] 사이에서 중복된 소절을 제거를 하고자 하였으나, 못함.\n",
    "#### 2. 또한 loss값이 제시된 2.2보다 낮은 걸로 보아, 오버피팅이 일어난 듯 싶음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
